{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport cupy as cp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\nfrom catboost import CatBoostRegressor\n\n\ncalendar_df = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv')\ninventory_df = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')\ntrain_df = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv')\ntest_df = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv')\ndf5 = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv')\nweights_df = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_columns = list(test_df.columns)\nkeep_columns =  list(train_df.columns)\nprint(test_columns)\nkeep_columns ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":" #Định dạng lại format date\ncalendar_df['date'] = pd.to_datetime(calendar_df['date'])\ntrain_df['date'] = pd.to_datetime(train_df['date'])\ntest_df['date'] = pd.to_datetime(test_df['date'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Frankfurt_1 = calendar_df.query('date >= \"2020-08-01 00:00:00\" and warehouse ==\"Frankfurt_1\"')\nPrague_2 = calendar_df.query('date >= \"2020-08-01 00:00:00\" and warehouse ==\"Prague_2\"')\nBrno_1 = calendar_df.query('date >= \"2020-08-01 00:00:00\" and warehouse ==\"Brno_1\"')\nMunich_1 = calendar_df.query('date >= \"2020-08-01 00:00:00\" and warehouse ==\"Munich_1\"')\nPrague_3 = calendar_df.query('date >= \"2020-08-01 00:00:00\" and warehouse ==\"Prague_3\"')\nPrague_1 = calendar_df.query('date >= \"2020-08-01 00:00:00\" and warehouse ==\"Prague_1\"')\nBudapest_1 = calendar_df.query('date >= \"2020-08-01 00:00:00\" and warehouse ==\"Budapest_1\"')\ndef process_calendar(df):\n    \"\"\"\n    - days_to_holiday\n    - days_to_shops_closed\n    - day_after_closing\n    - long_weekend\n    - weekday\n    - ...\n    \"\"\"\n    df = df.sort_values('date').reset_index(drop=True)\n\n\n    # 4. long_weekend\n    df['long_weekend'] = (\n        (df['shops_closed'] == 1) & (df['shops_closed'].shift(1) == 1)\n    ).astype(int)\n\n    # 5. weekday\n    df['weekday'] = df['date'].dt.weekday  # 0 (понедельник) - 6 (воскресенье)\n\n\n    # 7. holiday season\n    df['holiday_season'] = df['date'].apply(lambda x: 1 if (x.month == 12 and x.day >= 20) or (x.month == 1 and x.day <= 5) else 0)\n\n    # 8. week of month\n    df['week_of_month'] = df['date'].apply(lambda x: (x.day - 1) // 7 + 1)\n\n    # 9. quarter\n    df['quarter'] = df['date'].dt.quarter\n\n    # 10. is weekend\n    df['is_weekend'] = df['date'].dt.weekday.isin([5, 6]).astype(int)\n\n    return df\n\n\ndfs = ['Frankfurt_1', 'Prague_2', 'Brno_1', 'Munich_1', 'Prague_3', 'Prague_1', 'Budapest_1']\nprocessed_dfs = [process_calendar(globals()[df]) for df in dfs]\n\ncalendar_extended = pd.concat(processed_dfs).sort_values('date').reset_index(drop=True)\nprint(calendar_extended.isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_calendar = train_df.merge(calendar_extended, on=['date', 'warehouse'], how='left')\ntrain_inventory = train_calendar.merge(inventory_df, on=['unique_id', 'warehouse'], how='left')\ntrain_df = train_inventory.merge(weights_df, on=['unique_id'], how='left')\n\ntest_calendar = test_df.merge(calendar_extended, on=['date', 'warehouse'], how='left')\ntest_df = test_calendar.merge(inventory_df, on=['unique_id', 'warehouse'], how='left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of unique day:\",train_df['date'].nunique())\ntrain_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape\")\nprint(f\"train: {train_df.shape}\")\nprint(f\"test: {test_df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.loc[train_df.sales.isnull(),:].reset_index().groupby(['warehouse'],observed=False). \\\nagg(size=('warehouse','size'),\n    min_date=('date','min'),\n    max_date=('date','max'),\n    days = ('date', lambda x: x.max() - x.min()),\n    split_date=('date', lambda x: list(np.unique(np.unique(x.dt.strftime('%Y-%m-%d'))))) \n   ).dropna()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['sales'] = train_df['sales'].fillna(0)\ntrain_df['total_orders'] = train_df['total_orders'].fillna(0)\ntrain_df['sell_price_main'] = train_df['sell_price_main'].interpolate()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Số lượng NaN trong holiday_name theo holiday:\")\nprint(train_df.groupby('holiday')['holiday_name'].apply(lambda x: x.isna().sum()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kiểm tra tổng số hàng và số NaN trong holiday_name\nprint(\"Tổng số hàng trong train_df:\", len(train_df))\nprint(\"Số lượng NaN trong holiday_name:\", train_df['holiday_name'].isna().sum())\n\n# Kiểm tra số lượng NaN trong holiday_name theo holiday\nprint(\"\\nSố lượng NaN trong holiday_name theo holiday:\")\nprint(train_df.groupby('holiday')['holiday_name'].apply(lambda x: x.isna().sum()))\n\n# Lọc các ngày lễ (holiday == 1) có holiday_name là NaN\nmissing_holidays = train_df[(train_df['holiday'] == 1) & (train_df['holiday_name'].isna())][['date', 'warehouse', 'holiday', 'holiday_name']]\n\n# Hiển thị kết quả\nprint(\"\\nCác ngày lễ thiếu tên (holiday == 1, holiday_name là NaN):\")\nprint(missing_holidays)\n\n# Thống kê số lượng ngày lễ thiếu tên theo warehouse\nprint(\"\\nSố lượng ngày lễ thiếu tên theo warehouse:\")\nprint(missing_holidays.groupby('warehouse').size())\n\n# Thống kê các ngày duy nhất bị thiếu tên\nprint(\"\\nCác ngày duy nhất bị thiếu tên lễ:\")\nprint(missing_holidays['date'].dt.strftime('%Y-%m-%d').unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fill những ngày bị miss\n","metadata":{}},{"cell_type":"code","source":"# Danh sách ngày lễ (giữ nguyên từ code của bạn)\nczech_holiday = [\n    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),\n    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother's Day\"),\n]\nbrno_holiday = [\n    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),\n    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother's Day\"),\n]\nbudapest_holidays = []\nmunich_holidays = [\n    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),\n]\nfrank_holidays = [\n    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),\n    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], \"Mother's Day\"),\n]\n\n# Hàm fill_loss_holidays (giữ nguyên)\ndef fill_loss_holidays(df_fill, warehouses, holidays):\n    df = df_fill.copy()\n    for item in holidays:\n        dates, holiday_name = item\n        generated_dates = [datetime.strptime(date, '%m/%d/%Y').strftime('%Y-%m-%d') for date in dates]\n        for generated_date in generated_dates:\n            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday'] = 1\n            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday_name'] = holiday_name\n    return df\n\n# Kiểm tra NaN trong holiday_name trước khi xử lý\nprint(\"Số lượng NaN trong holiday_name (train_df) trước khi xử lý:\", train_df['holiday_name'].isna().sum())\nprint(\"\\nSố lượng NaN trong holiday_name theo holiday:\")\nprint(train_df.groupby('holiday')['holiday_name'].apply(lambda x: x.isna().sum()))\n\n# Điền ngày lễ vào train_df\ntrain_df = fill_loss_holidays(df_fill=train_df, warehouses=['Prague_1', 'Prague_2', 'Prague_3'], holidays=czech_holiday)\ntrain_df = fill_loss_holidays(df_fill=train_df, warehouses=['Brno_1'], holidays=brno_holiday)\ntrain_df = fill_loss_holidays(df_fill=train_df, warehouses=['Munich_1'], holidays=munich_holidays)\ntrain_df = fill_loss_holidays(df_fill=train_df, warehouses=['Frankfurt_1'], holidays=frank_holidays)\ntrain_df = fill_loss_holidays(df_fill=train_df, warehouses=['Budapest_1'], holidays=budapest_holidays)\n\n# Điền các NaN còn lại trong holiday_name bằng \"No Holiday\"\ntrain_df['holiday_name'] = train_df['holiday_name'].fillna(\"No Holiday\")\n\n# Kiểm tra NaN sau khi xử lý\nprint(\"\\nSố lượng NaN trong holiday_name (train_df) sau khi xử lý:\", train_df['holiday_name'].isna().sum())\nprint(\"\\nPhân bố giá trị trong holiday_name:\")\nprint(train_df['holiday_name'].value_counts())\nprint(\"\\nGiá trị holiday_name khi holiday == 1:\")\nprint(train_df[train_df['holiday'] == 1]['holiday_name'].value_counts())\n\n# Xử lý test_df (tương tự)\nprint(\"\\nSố lượng NaN trong holiday_name (test_df) trước khi xử lý:\", test_df['holiday_name'].isna().sum())\ntest_df = fill_loss_holidays(df_fill=test_df, warehouses=['Prague_1', 'Prague_2', 'Prague_3'], holidays=czech_holiday)\ntest_df = fill_loss_holidays(df_fill=test_df, warehouses=['Brno_1'], holidays=brno_holiday)\ntest_df = fill_loss_holidays(df_fill=test_df, warehouses=['Munich_1'], holidays=munich_holidays)\ntest_df = fill_loss_holidays(df_fill=test_df, warehouses=['Frankfurt_1'], holidays=frank_holidays)\ntest_df = fill_loss_holidays(df_fill=test_df, warehouses=['Budapest_1'], holidays=budapest_holidays)\n\n# Điền NaN còn lại trong test_df\ntest_df['holiday_name'] = test_df['holiday_name'].fillna(\"No Holiday\")\n\n# Kiểm tra NaN sau khi xử lý\nprint(\"\\nSố lượng NaN trong holiday_name (test_df) sau khi xử lý:\", test_df['holiday_name'].isna().sum())\nprint(\"\\nPhân bố giá trị trong holiday_name (test_df):\")\nprint(test_df['holiday_name'].value_counts())\n\n# # Lưu dữ liệu\n# train_df.to_csv('train_df_cleaned_holiday_name.csv', index=False)\n# test_df.to_csv('test_df_cleaned_holiday_name.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# xử lý outlier trong sales","metadata":{}},{"cell_type":"code","source":"Q1 = np.log1p(train_df[\"sales\"]).quantile(0.25)\nQ3 = np.log1p(train_df[\"sales\"]).quantile(0.75)\nIQR = Q3 - Q1\nlower = Q1 - 1.5 * IQR\nupper = Q3 + 1.5 * IQR\n\ntrain_df = train_df[(np.log1p(train_df[\"sales\"]) >= lower) & (np.log1p(train_df[\"sales\"]) <= upper)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6, 5))\n\nsns.histplot(x=np.log1p(train_df['sales']), bins=100, kde=True)\n\nplt.title(f'Log histplot of sales')\nplt.xlabel('sales')\nplt.ylabel('Frequency')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# xử lý discount <0 ","metadata":{}},{"cell_type":"code","source":"train_df.loc[train_df['type_0_discount'] < 0, 'type_0_discount'] = 0\ntrain_df.loc[train_df['type_4_discount'] < 0, 'type_4_discount'] = 0\ntrain_df.loc[train_df['type_6_discount'] < 0, 'type_6_discount'] = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ncols_to_scale = ['total_orders', 'sell_price_main']\n\nscaler = MinMaxScaler()\ntrain_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Kiểm tra các cột còn lại trong train_df\nprint(\"Các cột còn lại trong train_df:\", train_df.columns.tolist())\nprint(\"Danh sách cột giữ lại (keep_columns):\", keep_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"# Đảm bảo tqdm hoạt động với pandas\ntqdm.pandas()\n\n# Kiểm tra dữ liệu train_df trước khi xử lý\nprint(\"Kiểm tra NaN trong train_df trước khi xử lý:\")\nprint(train_df[['date', 'warehouse', 'total_orders']].isna().sum())\nprint(\"\\nĐịnh dạng cột date:\", train_df['date'].dtype)\n\n# Đảm bảo cột date là datetime\ntrain_df['date'] = pd.to_datetime(train_df['date'])\ntest_df['date'] = pd.to_datetime(test_df['date'])\n\n# Tính total_orders_median\ntrain_df[\"total_orders_median\"] = train_df.groupby([\"date\", \"warehouse\"])[\"total_orders\"].transform(\"median\")\n\n# Tính total_orders_med_diff\ntrain_df[\"total_orders_med_diff\"] = train_df[\"total_orders\"] - train_df[\"total_orders_median\"]\n\n# Tính max và min demand cho mỗi warehouse\ndf_warehouse_limits = train_df.groupby(\"warehouse\")[\"total_orders\"].agg([\"max\", \"min\"])\n\n# Kiểm tra trường hợp max = min để tránh chia cho 0\nprint(\"\\nKiểm tra max và min của total_orders theo warehouse:\")\nprint(df_warehouse_limits)\nif (df_warehouse_limits[\"max\"] == df_warehouse_limits[\"min\"]).any():\n    print(\"Cảnh báo: Một số warehouse có max = min, có thể gây lỗi khi tính warehouse_demand.\")\n\n# Tính warehouse_demand (sử dụng vector hóa thay vì progress_apply để tối ưu hiệu suất)\ntrain_df[\"warehouse_demand\"] = train_df.apply(\n    lambda x: (\n        (x[\"total_orders_median\"] - df_warehouse_limits.loc[x[\"warehouse\"], \"min\"]) /\n        (df_warehouse_limits.loc[x[\"warehouse\"], \"max\"] - df_warehouse_limits.loc[x[\"warehouse\"], \"min\"])\n    ) if (df_warehouse_limits.loc[x[\"warehouse\"], \"max\"] != df_warehouse_limits.loc[x[\"warehouse\"], \"min\"]) else 0,\n    axis=1\n)\n\n# Kiểm tra NaN trong các cột mới\nprint(\"\\nKiểm tra NaN trong train_df sau khi xử lý:\")\nprint(train_df[[\"total_orders_median\", \"total_orders_med_diff\", \"warehouse_demand\"]].isna().sum())\n\n# Kiểm tra phân bố của các cột mới\nprint(\"\\nThống kê mô tả cho các cột mới:\")\nprint(train_df[[\"total_orders_median\", \"total_orders_med_diff\", \"warehouse_demand\"]].describe())\n\n# Áp dụng tương tự cho test_df (nếu cần)\nprint(\"\\nKiểm tra NaN trong test_df trước khi xử lý:\")\nprint(test_df[['date', 'warehouse', 'total_orders']].isna().sum())\n\n# Tính total_orders_median cho test_df\ntest_df[\"total_orders_median\"] = test_df.groupby([\"date\", \"warehouse\"])[\"total_orders\"].transform(\"median\")\n\n# Tính total_orders_med_diff\ntest_df[\"total_orders_med_diff\"] = test_df[\"total_orders\"] - test_df[\"total_orders_median\"]\n\n# Tính warehouse_demand (sử dụng df_warehouse_limits từ train_df để đảm bảo chuẩn hóa nhất quán)\ntest_df[\"warehouse_demand\"] = test_df.apply(\n    lambda x: (\n        (x[\"total_orders_median\"] - df_warehouse_limits.loc[x[\"warehouse\"], \"min\"]) /\n        (df_warehouse_limits.loc[x[\"warehouse\"], \"max\"] - df_warehouse_limits.loc[x[\"warehouse\"], \"min\"])\n    ) if (df_warehouse_limits.loc[x[\"warehouse\"], \"max\"] != df_warehouse_limits.loc[x[\"warehouse\"], \"min\"]) else 0,\n    axis=1\n)\n\n# Kiểm tra NaN trong test_df\nprint(\"\\nKiểm tra NaN trong test_df sau khi xử lý:\")\nprint(test_df[[\"total_orders_median\", \"total_orders_med_diff\", \"warehouse_demand\"]].isna().sum())\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Total and average sales per category\ncategory_sales = train_df.groupby('L1_category_name_en')['sales'].agg(['sum', 'mean']).reset_index()\ncategory_sales.rename(columns={'sum': 'category_sales_sum', 'mean': 'category_sales_avg'}, inplace=True)\n# Merge into training and test datasets\ntrain_df = train_df.merge(category_sales, on='L1_category_name_en', how='left')\ntest_df = test_df.merge(category_sales, on='L1_category_name_en', how='left')\n\n# Convert date to datetime and extract components\nfor df in [train_df, test_df]:\n    df['date'] = pd.to_datetime(df['date'])\n    df['month'] = df['date'].dt.month\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['week_of_year'] = df['date'].dt.isocalendar().week\n\n# 1.Total and average orders per category\ncat_order_stats = train_df.groupby('L1_category_name_en')['total_orders'].agg(['sum','mean']).reset_index()\ncat_order_stats.rename(columns={'sum': 'category_orders_sum', 'mean': 'category_orders_avg'}, inplace=True)\n\n# 2. Merge các chỉ số này vào train_df\ntrain_df = train_df.merge(cat_order_stats,\n                          on='L1_category_name_en',\n                          how='left')\n\n# 3. Merge các chỉ số này vào test_df\ntest_df = test_df.merge(cat_order_stats,\n                        on='L1_category_name_en',\n                        how='left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the maximum discount applied in Sales_Train\ndiscount_columns = [f'type_{i}_discount' for i in range(7)]\ntrain_df['max_discount'] = train_df[discount_columns].max(axis=1)\n\n# Calculate the maximum discount applied in Sales_Test\ndiscount_columns_test = [f'type_{i}_discount' for i in range(7)]\ntest_df['max_discount'] = test_df[discount_columns_test].max(axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# Kiểm tra và chuyển cột date thành datetime\nif 'date' not in train_df.columns:\n    raise ValueError(\"Cột 'date' không tồn tại trong train_df\")\nif 'date' not in test_df.columns:\n    raise ValueError(\"Cột 'date' không tồn tại trong test_df\")\n\ntrain_df['date'] = pd.to_datetime(train_df['date'], errors='coerce')\ntest_df['date'] = pd.to_datetime(test_df['date'], errors='coerce')\n\n# Kiểm tra giá trị datetime hợp lệ\nif train_df['date'].isna().all():\n    raise ValueError(\"Cột 'date' trong train_df không chứa giá trị datetime hợp lệ\")\nif test_df['date'].isna().all():\n    raise ValueError(\"Cột 'date' trong test_df không chứa giá trị datetime hợp lệ\")\n\n# Thêm các đặc trưng sin và cos cho train_df\ntrain_df[\"day\"] = train_df[\"date\"].dt.day\ntrain_df[\"month\"] = train_df[\"date\"].dt.month\ntrain_df[\"year\"] = train_df[\"date\"].dt.year\ntrain_df['year_sin'] = np.sin(2 * np.pi * train_df['year'] / train_df['year'].max())\ntrain_df['year_cos'] = np.cos(2 * np.pi * train_df['year'] / train_df['year'].max())\ntrain_df['month_sin'] = np.sin(2 * np.pi * train_df['month'] / 12)\ntrain_df['month_cos'] = np.cos(2 * np.pi * train_df['month'] / 12)\ntrain_df['day_sin'] = np.sin(2 * np.pi * train_df['day'] / 31)\ntrain_df['day_cos'] = np.cos(2 * np.pi * train_df['day'] / 31)\n\n# Thêm các đặc trưng sin và cos cho test_df\ntest_df[\"day\"] = test_df[\"date\"].dt.day\ntest_df[\"month\"] = test_df[\"date\"].dt.month\ntest_df[\"year\"] = test_df[\"date\"].dt.year\ntest_df['year_sin'] = np.sin(2 * np.pi * test_df['year'] / train_df['year'].max())  # Sử dụng max từ train_df\ntest_df['year_cos'] = np.cos(2 * np.pi * test_df['year'] / train_df['year'].max())  # Sử dụng max từ train_df\ntest_df['month_sin'] = np.sin(2 * np.pi * test_df['month'] / 12)\ntest_df['month_cos'] = np.cos(2 * np.pi * test_df['month'] / 12)\ntest_df['day_sin'] = np.sin(2 * np.pi * test_df['day'] / 31)\ntest_df['day_cos'] = np.cos(2 * np.pi * test_df['day'] / 31)\n\n# Chuẩn hóa các đặc trưng sin và cos\ncols_to_scale = ['year_sin', 'year_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos']\nscaler = MinMaxScaler()\ntrain_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])\ntest_df[cols_to_scale] = scaler.transform(test_df[cols_to_scale])\n\n\n\n\n\n\n#Tuong tac giua warehouse và holiday_name\ntrain_df['warehouse_holiday'] = (train_df['warehouse'].astype(str) + '_' + train_df['holiday_name'].astype(str)).astype('category')\ntest_df['warehouse_holiday'] = (test_df['warehouse'].astype(str) + '_' + test_df['holiday_name'].astype(str)).astype('category')\n# Mã hóa lại\nle = LabelEncoder()\ncombined = np.concatenate([train_df['warehouse_holiday'].astype(str), test_df['warehouse_holiday'].astype(str)])\nle.fit(combined)\ntrain_df['warehouse_holiday'] = le.transform(train_df['warehouse_holiday'].astype(str)).astype(np.int32)\ntest_df['warehouse_holiday'] = le.transform(test_df['warehouse_holiday'].astype(str)).astype(np.int32)\n\n\n\n\n\n\n\n#Lag của total_orders (orders_lag):\nfor lag in [7, 14, 28]:\n    train_df[f'orders_lag_{lag}'] = train_df.groupby(['unique_id', 'warehouse'])['total_orders'].shift(lag).astype(np.float32)\n    test_df[f'orders_lag_{lag}'] = test_df.groupby(['unique_id', 'warehouse'])['total_orders'].shift(lag).astype(np.float32)\n\n\n\n\n# Kiểm tra kết quả\nprint(\"Cột trong train_df sau khi thêm đặc trưng:\", train_df.columns.tolist())\nprint(\"Cột trong test_df sau khi thêm đặc trưng:\", test_df.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# keep_columns.append('category_sales_sum')\n# keep_columns.append('category_sales_avg')\n# keep_columns.append('category_orders_sum')\n# keep_columns.append('category_orders_avg')\n# keep_columns.append('total_discount')\n# keep_columns.append('L1_category_name_en')\n# train_df = train_df[keep_columns]\ntrain_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols_to_scale = ['category_sales_sum', 'category_sales_avg','category_orders_sum', 'category_orders_avg']\n\nscaler = MinMaxScaler()\ntrain_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Định nghĩa lag_features (có thể điều chỉnh)\nlag_features = [1, 7, 14, 28]  # Lag 1, 2, 3, 7 ngày\n\n\n\n# Đảm bảo cột date là datetime\ntrain_df['date'] = pd.to_datetime(train_df['date'])\ntest_df['date'] = pd.to_datetime(test_df['date'])\n\n# --- Xử lý train_df ---\n\n# Nhóm dữ liệu theo unique_id và warehouse\ndf_grouped = train_df.groupby([\"unique_id\", \"warehouse\"])\n\n# Tính lag features cho train_df\nfor i in lag_features:\n    # Tạo cột lag\n    train_df[f\"sales_item_warehouse_lag_{i}\"] = df_grouped[\"sales\"].shift(i)\n    # Điền NaN bằng giá trị sales cuối cùng của nhóm, nếu không có thì điền 0\n    train_df[f\"sales_item_warehouse_lag_{i}\"] = train_df[f\"sales_item_warehouse_lag_{i}\"].fillna(\n        df_grouped[\"sales\"].transform(\"last\")\n    ).fillna(0)\n\n# Kiểm tra NaN trong các cột lag của train_df\nprint(\"\\nKiểm tra NaN trong các cột lag của train_df:\")\nprint(train_df[[f\"sales_item_warehouse_lag_{i}\" for i in lag_features]].isna().sum())\n\n# Kiểm tra phân bố của các cột lag\nprint(\"\\nThống kê mô tả cho các cột lag trong train_df:\")\nprint(train_df[[f\"sales_item_warehouse_lag_{i}\" for i in lag_features]].describe())\n\n# --- Xử lý test_df ---\n\n# Tạo DataFrame kết hợp train_df và test_df để tính lag liên tục\n# Chỉ lấy các cột cần thiết từ train_df\ntrain_subset = train_df[[\"date\", \"unique_id\", \"warehouse\", \"sales\"]].copy()\n# Tạo cột sales giả cho test_df (đặt là NaN vì test_df không có sales)\ntest_subset = test_df[[\"date\", \"unique_id\", \"warehouse\"]].copy()\ntest_subset[\"sales\"] = np.nan\n\n# Kết hợp train_df và test_df\ncombined_df = pd.concat([train_subset, test_subset], ignore_index=True)\ncombined_df = combined_df.sort_values([\"unique_id\", \"warehouse\", \"date\"])\n\n# Nhóm dữ liệu kết hợp\ncombined_grouped = combined_df.groupby([\"unique_id\", \"warehouse\"])\n\n# Tính lag features cho combined_df\nfor i in lag_features:\n    combined_df[f\"sales_item_warehouse_lag_{i}\"] = combined_grouped[\"sales\"].shift(i)\n    # Điền NaN bằng giá trị sales cuối cùng của nhóm, nếu không có thì điền 0\n    combined_df[f\"sales_item_warehouse_lag_{i}\"] = combined_df[f\"sales_item_warehouse_lag_{i}\"].fillna(\n        combined_grouped[\"sales\"].transform(\"last\")\n    ).fillna(0)\n\n# Lấy các cột lag cho test_df\ntest_lag_features = combined_df[combined_df[\"date\"].isin(test_df[\"date\"])][\n    [\"date\", \"unique_id\", \"warehouse\"] + [f\"sales_item_warehouse_lag_{i}\" for i in lag_features]\n]\n\n# Gộp lag features vào test_df\ntest_df = test_df.merge(\n    test_lag_features[[\"date\", \"unique_id\"] + [f\"sales_item_warehouse_lag_{i}\" for i in lag_features]],\n    on=[\"date\", \"unique_id\"],\n    how=\"left\"\n)\n\n# Kiểm tra NaN trong các cột lag của test_df\nprint(\"\\nKiểm tra NaN trong các cột lag của test_df:\")\nprint(test_df[[f\"sales_item_warehouse_lag_{i}\" for i in lag_features]].isna().sum())\n\n# Kiểm tra phân bố của các cột lag trong test_df\nprint(\"\\nThống kê mô tả cho các cột lag trong test_df:\")\nprint(test_df[[f\"sales_item_warehouse_lag_{i}\" for i in lag_features]].describe())\n\n# Lưu dữ liệu\ntrain_df.to_csv('train_df_with_lags.csv', index=False)\ntest_df.to_csv('test_df_with_lags.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 12))\ncorr_matrix = train_df.corr(numeric_only=True).abs()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Phân tích tương quan với sales\ncorr_with_sales = corr_matrix['sales'].abs().sort_values(ascending=False)\nprint(\"Tương quan tuyệt đối với sales:\")\nprint(corr_with_sales)\n\n# Vẽ biểu đồ phân phối tương quan\nplt.figure(figsize=(10, 6))\nplt.hist(corr_with_sales.drop('sales'), bins=20)\nplt.xlabel('Tương quan tuyệt đối với sales')\nplt.ylabel('Số lượng đặc trưng')\nplt.title('Phân phối tương quan với sales')\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoders = {}\nfor feature in [\"warehouse\", \"holiday_name\"]:\n    le = LabelEncoder()\n    df[feature] = le.fit_transform(df[feature])\n    label_encoders[feature] = le","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # Loại bỏ cột weight và availability\n    columns_to_drop = ['weight', 'availability']\n    train_df = train_df.drop(columns=[col for col in columns_to_drop if col in train_df.columns], errors='ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nNote 12:\n    Not ideal for all solutions, but didn't explore this in depth yet\n\"\"\"\ntrain_df[\"sales\"] = train_df[\"sales\"].fillna(0)\ntrain_df[\"warehouse_demand\"] = train_df[\"warehouse_demand\"].fillna(0)\ntrain_df = train_df.fillna(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install catboost","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Giả sử train_df, test_df, weights_df đã được chuẩn bị\n# Tối ưu hóa kiểu dữ liệu (như trong pipeline)\nfor col in train_df.select_dtypes('float64'):\n    train_df[col] = train_df[col].astype('float32')\nfor col in train_df.select_dtypes('int64'):\n    train_df[col] = train_df[col].astype('int32')\nfor col in train_df.select_dtypes('bool'):\n    train_df[col] = train_df[col].astype('uint8')\n\nfor col in test_df.select_dtypes('float64'):\n    test_df[col] = test_df[col].astype('float32')\nfor col in test_df.select_dtypes('int64'):\n    test_df[col] = test_df[col].astype('int32')\nfor col in test_df.select_dtypes('bool'):\n    test_df[col] = test_df[col].astype('uint8')\n# Tiền xử lý các cột object\ncategorical_cols = ['warehouse', 'holiday_name', 'name', 'L1_category_name_en', \n                    'L2_category_name_en', 'L3_category_name_en', 'L4_category_name_en']\n# Áp dụng Label Encoding, giảm bản sao\nlabel_encoders = {}\nfor col in categorical_cols:\n    if col in train_df.columns and col in test_df.columns:\n        le = LabelEncoder()\n        # Xử lý từng cột để giảm RAM\n        train_vals = train_df[col].astype(str).values\n        test_vals = test_df[col].astype(str).values\n        combined = np.concatenate([train_vals, test_vals])\n        le.fit(combined)\n        train_df[col] = le.transform(train_vals)\n        test_df[col] = le.transform(test_vals)\n        label_encoders[col] = le\n        del combined  # Xóa biến tạm\n\n# # Hàm inverse_norm\n# def inverse_norm(df, period, values):\n#     sales_min = train_df[\"sales\"].min()\n#     sales_max = train_df[\"sales\"].max()\n#     return values * (sales_max - sales_min) + sales_min\n\n\n# Định nghĩa features dựa trên cả train_df và test_df\n# Loại bỏ cột datetime64 và chỉ giữ cột số\ntrain_features = [c for c in train_df.columns if c not in [\"unique_id\", \"date\", \"sales\", \"availability\"] \n                 and train_df[c].dtype in [np.float32, np.int32, np.uint8, np.int64]]\ntest_features = [c for c in test_df.columns if c not in [\"unique_id\", \"date\", \"sales\", \"availability\"] \n                 and test_df[c].dtype in [np.float32, np.int32, np.uint8, np.int64]]\n# Lấy tập hợp chung của features\nfeatures = list(set(train_features) & set(test_features))\n\n# Kiểm tra kiểu dữ liệu của features\nprint(\"Feature dtypes in train_df:\")\nfor col in features:\n    print(f\"{col}: {train_df[col].dtype}\")\nprint(\"\\nFeature dtypes in test_df:\")\nfor col in features:\n    print(f\"{col}: {test_df[col].dtype}\")\n\n\ntarget = \"sales\"\ntraining_dates = (train_df[\"date\"].min(), train_df[\"date\"].max() - pd.Timedelta(days=28))\nvalidation_dates = (training_dates[1] + pd.Timedelta(days=1), training_dates[1] + pd.Timedelta(days=14))\ntest_dates = (test_df[\"date\"].min(), test_df[\"date\"].max())\nweight_map = weights_df.set_index('unique_id')['weight'].to_dict()\n\n# Chuẩn bị dữ liệu\nX_train = train_df[train_df[\"date\"].between(*training_dates)][features]\ny_train = train_df[train_df[\"date\"].between(*training_dates)][target]\nX_val = train_df[train_df[\"date\"].between(*validation_dates)][features]\ny_val = train_df[train_df[\"date\"].between(*validation_dates)][target]\nunique_id_val = train_df[train_df[\"date\"].between(*validation_dates)][\"unique_id\"]\n# Kiểm tra NaN\nif X_train.isna().any().any() or X_val.isna().any().any():\n    print(\"NaN detected in input data. Filling remaining NaNs with mean.\")\n    X_train = X_train.fillna(X_train.mean())\n    X_val = X_val.fillna(X_val.mean())# Kiểm tra NaN\nif X_train.isna().any().any() or X_val.isna().any().any():\n    print(\"NaN detected in input data. Filling remaining NaNs with mean.\")\n    X_train = X_train.fillna(X_train.mean())\n    X_val = X_val.fillna(X_val.mean())\n# Kiểm tra weight_map\nmissing_ids = set(unique_id_val) - set(weight_map.keys())\nif missing_ids:\n    print(f\"Missing unique_id in weight_map: {missing_ids}\")\n    # Gán trọng số mặc định = 1 cho các unique_id thiếu\n    for missing_id in missing_ids:\n        weight_map[missing_id] = 1.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chuyển dữ liệu sang GPU\nX_train_gpu = cp.array(X_train.values, dtype=cp.float32)\ny_train_gpu = cp.array(y_train.values, dtype=cp.float32)\nX_val_gpu = cp.array(X_val.values, dtype=cp.float32)\n\n# Huấn luyện mô hình XGBoost với GridSearchCV và GPU\nxgb = XGBRegressor(\n    objective='reg:absoluteerror',\n    random_state=2025,\n    use_label_encoder=False,\n    tree_method='hist',\n    device='cuda'  # Chạy trên GPU\n)\nxgb_param_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.05, 0.1]\n}\nxgb_gs = GridSearchCV(\n    xgb,\n    xgb_param_grid,\n    cv=3,\n    scoring='neg_mean_absolute_error',\n    n_jobs=1,  # Tắt song song để giảm RAM\n    verbose=1\n)\n\n# Huấn luyện và kiểm tra lỗi\ntry:\n    xgb_gs.fit(X_train_gpu.get(), y_train_gpu.get())  # Chuyển về numpy cho GridSearchCV\n    print(\"GridSearchCV completed successfully.\")\nexcept Exception as e:\n    print(f\"Error during GridSearchCV fit: {e}\")\n    raise\n\n\n\ny_pred_xgb = cp.asnumpy(xgb_gs.predict(X_val_gpu)) \n\n# Đánh giá mô hình\nwmae = mean_absolute_error(y_val, y_pred_xgb, \n                          sample_weight=unique_id_val.map(weight_map).values)\n\n# In kết quả\nprint(f\"Best Parameters: {xgb_gs.best_params_}\")\nprint(f\"Validation WMAE: {wmae}\")\nprint(f\"Test Predictions: {y_pred_xgb}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 5. Ridge Regression ===\nridge_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('ridge', Ridge())\n])\nridge_param_grid = {\n    'ridge__alpha': [0.1, 1, 10, 100],\n    'ridge__solver': ['auto', 'lsqr'],\n    'ridge__fit_intercept': [True]\n}\nridge_gs = GridSearchCV(\n    ridge_pipe,\n    ridge_param_grid,\n    cv=5,\n    n_jobs=1,\n    scoring='neg_mean_absolute_error', \n    verbose=1\n)\nridge_gs.fit(X_train, y_train)\n# Dự đoán trên tập xác thực và kiểm tra\ny_pred_ridge = ridge_gs.predict(X_val)\n\n# Đánh giá mô hình\nwmae = mean_absolute_error(y_val, y_pred_ridge, \n                          sample_weight=unique_id_val.map(weight_map).values)\n\n# In kết quả\nprint(f\"Best Parameters: {ridge_gs.best_params_}\")\nprint(f\"Validation WMAE: {wmae}\")\nprint(f\"Test Predictions: {y_pred_ridge}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chuyển dữ liệu sang GPU\nX_train_np = X_train.values.astype(np.float32)\ny_train_np = y_train.values.astype(np.float32)\nX_val_np = X_val.values.astype(np.float32)\n\n\n\n# === 6. LightGBM (GridSearch) ===\nlgb = LGBMRegressor(objective= 'regression', random_state=43, force_row_wise=True, verbose=-1, device='gpu')\nlgb_param_dist = {\n    'n_estimators': [1000],\n    'max_depth': [10],\n    'learning_rate': [0.054],\n    'num_leaves': [273],\n    'min_child_samples': [40],\n    'colsample_bytree' : [0.852], \n    'colsample_bynode' : [0.72], \n    'min_data_in_leaf' : [6],\n    'reg_alpha': [0.056],\n    'num_boost_round': [11000],\n    'reg_lambda': [0.4]\n}\nlgb_gs = GridSearchCV(\n    lgb, lgb_param_dist,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    n_jobs=1, verbose=1)\n# Huấn luyện\ntry:\n    lgb_gs.fit(X_train_np, y_train_np)  # Chuyển về numpy cho RandomizedSearchCV\n    print(\"GridSearchCV completed successfully.\")\nexcept Exception as e:\n    print(f\"Error during RandomizedSearchCV fit: {e}\")\n    raise\n\n\ny_pred_lgb = cp.asnumpy(lgb_gs.predict(X_val_np)) \n# Đánh giá mô hình\nwmae = mean_absolute_error(y_val, y_pred_lgb, \n                          sample_weight=unique_id_val.map(weight_map).values)\n\n# In kết quả\nprint(f\"Best Parameters: {lgb_gs.best_params_}\")\nprint(f\"Validation WMAE: {wmae}\")\nprint(f\"Test Predictions: {y_pred_lgb}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 7. ElasticNet ===\nen_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('enet', ElasticNet(random_state=43))\n])\nen_param_grid = {\n    'enet__alpha': [0.01, 0.1, 1],\n    'enet__l1_ratio': [0.1, 0.5, 0.9],\n    'enet__fit_intercept': [True, False],\n    'enet__max_iter': [1000]\n}\nen_gs = GridSearchCV(\n    en_pipe,\n    en_param_grid,\n    cv=3,\n    scoring='neg_mean_absolute_error',\n    n_jobs=1,\n    verbose=1\n)\nen_gs.fit(X_train, y_train)\ny_pred_en = en_gs.predict(X_val)\n# Đánh giá mô hình\nwmae = mean_absolute_error(y_val, y_pred_en, \n                          sample_weight=unique_id_val.map(weight_map).values)\n\n# In kết quả\nprint(f\"Best Parameters: {en_gs.best_params_}\")\nprint(f\"Validation WMAE: {wmae}\")\nprint(f\"Test Predictions: {y_pred_en}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#8. catboost\n# Huấn luyện mô hình CatBoost với GridSearchCV và GPU\nX_train_np = X_train.values.astype(np.float32)\ny_train_np = y_train.values.astype(np.float32)\nX_val_np = X_val.values.astype(np.float32)\n\n\n# Huấn luyện mô hình CatBoost với GridSearchCV và GPU\ncat = CatBoostRegressor(random_seed=43, verbose=0, task_type='GPU', grow_policy='Lossguide')\ncat_param_grid = {\n    'iterations': [500, 1000],\n    'depth': [8, 10],\n    'bagging_temperature': [0.5],\n    'learning_rate': [0.05, 0.1],\n    'max_leaves': [128],\n    'l2_leaf_reg': [2],\n    'min_data_in_leaf': [24]\n}\ncat_gs = GridSearchCV(\n    cat,\n    cat_param_grid,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    n_jobs=1,  \n    verbose=1\n)\n\n\ntry:\n    cat_gs.fit(X_train_np, y_train_np) \n    print(\"GridSearchCV completed successfully.\")\nexcept Exception as e:\n    print(f\"Error during RandomizedSearchCV fit: {e}\")\n    raise\n\ny_pred_cat = cat_gs.predict(X_val_np)\n# Đánh giá mô hình\nwmae = mean_absolute_error(y_val, y_pred_cat, \n                          sample_weight=unique_id_val.map(weight_map).values)\n\n# In kết quả\nprint(f\"Best Parameters: {cat_gs.best_params_}\")\nprint(f\"Validation WMAE: {wmae}\")\nprint(f\"Test Predictions: {y_pred_cat}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keep_columns_1=train_df.columns\nkeep_columns_1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Identify missing columns\nmissing_cols = list(set(train_df.columns) - set(test_df.columns))\nmissing_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keep_columns=train_df.columns\nkeep_columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 2. Mã hóa và chuyển đổi date\n# le = LabelEncoder()\n# # test_df['L1_category_name_en'] = le.fit_transform(test_df['L1_category_name_en'])\n# # test_df['holiday_name'] = le.fit_transform(test_df['holiday_name'])  # Apply Label Encoding\n# test_df['name'] = le.fit_transform(test_df['name'])\n# test_df['L2_category_name_en'] = le.fit_transform(test_df['L2_category_name_en'].astype(str))\n# test_df['L3_category_name_en'] = le.fit_transform(test_df['L3_category_name_en'].astype(str))\n# test_df['L4_category_name_en'] = le.fit_transform(test_df['L4_category_name_en'].astype(str))\n\n\n# # test_df['date'] = pd.to_datetime(test_df['date'])\n# # test_df['date'] = (test_df['date'] - test_df['date'].min()).dt.days\n# for col in test_df.select_dtypes('float64'): test_df[col] = test_df[col].astype('float32')\n# for col in test_df.select_dtypes('int64'):   test_df[col] = test_df[col].astype('int32')\n# for col in test_df.select_dtypes('bool'):    test_df[col] = test_df[col].astype('uint8')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Add missing columns to test_df with default values (e.g., 0)\nfor col in missing_cols:\n    test_df[col] = 0  # Or use np.nan or another suitable default\n\n# 3. Select the desired columns\ntest_df = test_df[keep_columns]\ntest_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dự đoán trên test_df\ntry:\n    X_test_new = test_df[test_df[\"date\"].between(*test_dates)][features]  # Lọc theo test_dates\n    X_test_new_np = X_test_new.values.astype(np.float32)\n    y_pred_test = lgb_gs.predict(X_test_new_np)\n    # Nếu cần inverse_norm, bỏ comment dòng dưới\n    # y_pred_test = inverse_norm(test_df, y_pred_test)\n    test_df.loc[test_df[\"date\"].between(*test_dates), 'sales'] = y_pred_test\nexcept Exception as e:\n    print(f\"Error during test prediction: {e}\")\n    raise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solution = pd.DataFrame({\n    'id': df5['id'],       # id từ df5\n    'sales_hat': test_df['sales']  # sales từ test_df\n})\n\n# Lưu kết quả vào file CSV\nsolution.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(solution)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}